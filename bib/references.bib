
@article{bottcher_gradient-free_2023,
	title = {Gradient-free training of neural {ODEs} for system identification and control using ensemble {Kalman} inversion},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2307.07882},
	doi = {10.48550/ARXIV.2307.07882},
	abstract = {Ensemble Kalman inversion (EKI) is a sequential Monte Carlo method used to solve inverse problems within a Bayesian framework. Unlike backpropagation, EKI is a gradient-free optimization method that only necessitates the evaluation of artificial neural networks in forward passes. In this study, we examine the effectiveness of EKI in training neural ordinary differential equations (neural ODEs) for system identification and control tasks. To apply EKI to optimal control problems, we formulate inverse problems that incorporate a Tikhonov-type regularization term. Our numerical results demonstrate that EKI is an efficient method for training neural ODEs in system identification and optimal control problems, with runtime and quality of solutions that are competitive with commonly used gradient-based optimizers.},
	urldate = {2024-05-17},
	author = {Böttcher, Lucas},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {type: Journal_Article, model: Dynamic,domain: Neural_Network, evaluation: RMSE, Computational Physics (physics.comp-ph), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Mathematics, FOS: Physical sciences, Machine Learning (cs.LG), Numerical Analysis (math.NA), Systems and Control (eess.SY)},
	annote = {Other
10 pages, 3 figures, Workshop on New Frontiers in Learning, Control, and Dynamical Systems at the International Conference on Machine Learning (ICML), Honolulu, Hawaii, USA, 2023},
}

@article{chada_iterative_2021,
	title = {Iterative ensemble {Kalman} methods: {A} unified perspective with some new variants},
	volume = {3},
	issn = {2639-8001},
	shorttitle = {Iterative ensemble {Kalman} methods},
	url = {https://www.aimsciences.org/article/doi/10.3934/fods.2021011},
	doi = {10.3934/fods.2021011},
	abstract = {{\textless}p style='text-indent:20px;'{\textgreater}This paper provides a unified perspective of iterative ensemble Kalman methods, a family of derivative-free algorithms for parameter reconstruction and other related tasks. We identify, compare and develop three subfamilies of ensemble methods that differ in the objective they seek to minimize and the derivative-based optimization scheme they approximate through the ensemble. Our work emphasizes two principles for the derivation and analysis of iterative ensemble Kalman methods: statistical linearization and continuum limits. Following these guiding principles, we introduce new iterative ensemble Kalman methods that show promising numerical performance in Bayesian inverse problems, data assimilation and machine learning tasks.{\textless}/p{\textgreater}},
	number = {3},
	urldate = {2024-05-17},
	journal = {Foundations of Data Science},
	author = {Chada, Neil K. and Chen, Yuming and Sanz-Alonso, Daniel},
	year = {2021},
	pages = {331},
	file = {Submitted Version:/Users/tejas/Zotero/storage/HTZ9QKDE/Chada et al. - 2021 - Iterative ensemble Kalman methods A unified persp.pdf:application/pdf},
	keywords = {type: Journal_Article, model: Iterative_Ensemble, domain: Methodological_Applications, evaluation: Parameter_Effectiveness}
}

@inproceedings{chen_approximate_2019,
	address = {Budapest, Hungary},
	title = {Approximate {Bayesian} {Neural} {Network} {Trained} with {Ensemble} {Kalman} {Filter}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72811-985-4},
	url = {https://ieeexplore.ieee.org/document/8851742/},
	doi = {10.1109/IJCNN.2019.8851742},
	urldate = {2024-05-17},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Chen, Chao and Lin, Xiao and Huang, Yuan and Terejanu, Gabriel},
	month = jul,
	year = {2019},
	pages = {1--8},
	keywords = {type: Conference_Paper, model: Baysian_NN, domain: Neural_Network, evaluation: RMSE}
}

@article{chen_novel_2020,
	title = {A {Novel} {Neural} {Network} {Training} {Framework} with {Data} {Assimilation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2010.02626},
	doi = {10.48550/ARXIV.2010.02626},
	abstract = {In recent years, the prosperity of deep learning has revolutionized the Artificial Neural Networks. However, the dependence of gradients and the offline training mechanism in the learning algorithms prevents the ANN for further improvement. In this study, a gradient-free training framework based on data assimilation is proposed to avoid the calculation of gradients. In data assimilation algorithms, the error covariance between the forecasts and observations is used to optimize the parameters. Feedforward Neural Networks (FNNs) are trained by gradient decent, data assimilation algorithms (Ensemble Kalman Filter (EnKF) and Ensemble Smoother with Multiple Data Assimilation (ESMDA)), respectively. ESMDA trains FNN with pre-defined iterations by updating the parameters using all the available observations which can be regard as offline learning. EnKF optimize FNN when new observation available by updating parameters which can be regard as online learning. Two synthetic cases with the regression of a Sine Function and a Mexican Hat function are assumed to validate the effectiveness of the proposed framework. The Root Mean Square Error (RMSE) and coefficient of determination (R2) are used as criteria to assess the performance of different methods. The results show that the proposed training framework performed better than the gradient decent method. The proposed framework provides alternatives for online/offline training the existing ANNs (e.g., Convolutional Neural Networks, Recurrent Neural Networks) without the dependence of gradients.},
	urldate = {2024-05-17},
	author = {Chen, Chong and Xing, Qinghui and Ding, Xin and Xue, Yaru and Zhong, Tianfu},
	year = {2020},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {type: Journal_Article, model: Feedforward, domain: Methodological_Applications ,evaluation: RMSE ,Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{haber_never_2018,
	title = {Never look back - {A} modified {EnKF} method and its application to the training of neural networks without back propagation},
	url = {http://arxiv.org/abs/1805.08034},
	abstract = {In this work, we present a new derivative-free optimization method and investigate its use for training neural networks. Our method is motivated by the Ensemble Kalman Filter (EnKF), which has been used successfully for solving optimization problems that involve large-scale, highly nonlinear dynamical systems. A key benefit of the EnKF method is that it requires only the evaluation of the forward propagation but not its derivatives. Hence, in the context of neural networks, it alleviates the need for back propagation and reduces the memory consumption dramatically. However, the method is not a pure "black-box" global optimization heuristic as it efficiently utilizes the structure of typical learning problems. Promising first results of the EnKF for training deep neural networks have been presented recently by Kovachki and Stuart. We propose an important modification of the EnKF that enables us to prove convergence of our method to the minimizer of a strongly convex function. Our method also bears similarity with implicit filtering and we demonstrate its potential for minimizing highly oscillatory functions using a simple example. Further, we provide numerical examples that demonstrate the potential of our method for training deep neural networks.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Haber, Eldad and Lucka, Felix and Ruthotto, Lars},
	month = may,
	year = {2018},
	note = {arXiv:1805.08034 [cs, math]},
	keywords = {type: Preprint, model: DNN, domain: Neural_Network ,evaluation: Accuracy, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Numerical Analysis},
	annote = {Comment: 10 pages, 2 figures},
	file = {arXiv Fulltext PDF:/Users/tejas/Zotero/storage/4PRWIPK9/Haber et al. - 2018 - Never look back - A modified EnKF method and its a.pdf:application/pdf;arXiv.org Snapshot:/Users/tejas/Zotero/storage/9E3F6ZXD/1805.html:text/html},
}

@article{kovachki_ensemble_2019,
	title = {Ensemble {Kalman} inversion: a derivative-free technique for machine learning tasks},
	volume = {35},
	issn = {0266-5611, 1361-6420},
	shorttitle = {Ensemble {Kalman} inversion},
	url = {https://iopscience.iop.org/article/10.1088/1361-6420/ab1c3a},
	doi = {10.1088/1361-6420/ab1c3a},
	number = {9},
	urldate = {2024-05-17},
	journal = {Inverse Problems},
	author = {Kovachki, Nikola B and Stuart, Andrew M},
	month = sep,
	year = {2019},
	pages = {095005},
	file = {model: DNN, domain: Neural_Network, Accepted Version:/Users/tejas/Zotero/storage/I9Z9VL7Z/Kovachki and Stuart - 2019 - Ensemble Kalman inversion a derivative-free techn.pdf:application/pdf},
	keywords = {type: Journal_Article, domain: Neural_Network, evaluation: Accuracy}
}

@article{lopezgomez_training_2022,
	title = {Training {Physics}‐{Based} {Machine}‐{Learning} {Parameterizations} {With} {Gradient}‐{Free} {Ensemble} {Kalman} {Methods}},
	volume = {14},
	issn = {1942-2466, 1942-2466},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2022MS003105},
	doi = {10.1029/2022MS003105},
	abstract = {Abstract
            Most machine learning applications in Earth system modeling currently rely on gradient‐based supervised learning. This imposes stringent constraints on the nature of the data used for training (typically, residual time tendencies are needed), and it complicates learning about the interactions between machine‐learned parameterizations and other components of an Earth system model. Approaching learning about process‐based parameterizations as an inverse problem resolves many of these issues, since it allows parameterizations to be trained with partial observations or statistics that directly relate to quantities of interest in long‐term climate projections. Here, we demonstrate the effectiveness of Kalman inversion methods in treating learning about parameterizations as an inverse problem. We consider two different algorithms: unscented and ensemble Kalman inversion. Both methods involve highly parallelizable forward model evaluations, converge exponentially fast, and do not require gradient computations. In addition, unscented Kalman inversion provides a measure of parameter uncertainty. We illustrate how training parameterizations can be posed as a regularized inverse problem and solved by ensemble Kalman methods through the calibration of an eddy‐diffusivity mass‐flux scheme for subgrid‐scale turbulence and convection, using data generated by large‐eddy simulations. We find the algorithms amenable to batching strategies, robust to noise and model failures, and efficient in the calibration of hybrid parameterizations that can include empirical closures and neural networks.
          , 
            Plain Language Summary
            Artificial intelligence represents an exciting opportunity in Earth system modeling, but its application brings its own set of challenges. One of the challenges is to train machine learning systems within Earth system models from partial or indirect data. Here, we present algorithms, known as ensemble Kalman methods, which can be used to train such systems. We demonstrate their use in situations where the data used for training are noisy, only indirectly informative about the model to be trained, and may only become available sequentially. As an example, we present training results for a state‐of‐the‐art model for turbulence, convection, and clouds for use within Earth system models. This model is shown to learn efficiently from data in a variety of configurations, including situations where the model contains neural networks.
          , 
            Key Points
            
              
                
                  Ensemble Kalman methods can be used to train parameterizations regardless of their architecture
                
                
                  They enable learning from partial observations or statistics in the presence of noise
                
                
                  Their effectiveness is demonstrated by calibrating an atmospheric turbulence and convection scheme},
	language = {en},
	number = {8},
	urldate = {2024-05-17},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Lopez‐Gomez, Ignacio and Christopoulos, Costa and Langeland Ervik, Haakon Ludvig and Dunbar, Oliver R. A. and Cohen, Yair and Schneider, Tapio},
	month = aug,
	year = {2022},
	pages = {e2022MS003105},
	file = {Accepted Version:/Users/tejas/Zotero/storage/LEYQ76JN/Lopez‐Gomez et al. - 2022 - Training Physics‐Based Machine‐Learning Parameteri.pdf:application/pdf},
	keywords = {type: Journal_Article, model: Physical_NN, domain: Physical_Hybrid_Models, evaluation: MSE}
}

@inproceedings{mirikitani_dynamic_2008,
	address = {San Diego, CA, USA},
	title = {Dynamic {Modeling} with {Ensemble} {Kalman} {Filter} {Trained} {Recurrent} {Neural} {Networks}},
	isbn = {978-0-7695-3495-4},
	url = {http://ieeexplore.ieee.org/document/4725078/},
	doi = {10.1109/ICMLA.2008.79},
	urldate = {2024-05-17},
	booktitle = {2008 {Seventh} {International} {Conference} on {Machine} {Learning} and {Applications}},
	publisher = {IEEE},
	author = {Mirikitani, Derrick T. and Nikolaev, Nikolay},
	year = {2008},
	pages = {843--848},
	keywords = {type: Conference_Paper, model: Dynamic, domain: Neural_Network ,evaluation: MSE}
}

@incollection{nicosia_ensemble_2020,
	address = {Cham},
	title = {Ensemble {Kalman} {Filter} {Optimizing} {Deep} {Neural} {Networks}: {An} {Alternative} {Approach} to {Non}-performing {Gradient} {Descent}},
	volume = {12566},
	isbn = {978-3-030-64579-3 978-3-030-64580-9},
	shorttitle = {Ensemble {Kalman} {Filter} {Optimizing} {Deep} {Neural} {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-64580-9_7},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {Machine {Learning}, {Optimization}, and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Yegenoglu, Alper and Krajsek, Kai and Pier, Sandra Diaz and Herty, Michael},
	editor = {Nicosia, Giuseppe and Ojha, Varun and La Malfa, Emanuele and Jansen, Giorgio and Sciacca, Vincenzo and Pardalos, Panos and Giuffrida, Giovanni and Umeton, Renato},
	year = {2020},
	doi = {10.1007/978-3-030-64580-9_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {78--92},
	keywords = {type: Book_Section, Conference_Paper, model: DNN, domain: Neural_Network, evaluation: MSE}
}

@article{zhang_ensemble_2022,
	title = {Ensemble {Kalman} method for learning turbulence models from indirect observation data},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2202.05122},
	doi = {10.48550/ARXIV.2202.05122},
	abstract = {In this work, we propose using an ensemble Kalman method to learn a nonlinear eddy viscosity model, represented as a tensor basis neural network, from velocity data. Data-driven turbulence models have emerged as a promising alternative to traditional models for providing closure mapping from the mean velocities to Reynolds stresses. Most data-driven models in this category need full-field Reynolds stress data for training, which not only places stringent demand on the data generation but also makes the trained model ill-conditioned and lacks robustness. This difficulty can be alleviated by incorporating the Reynolds-averaged Navier-Stokes (RANS) solver in the training process. However, this would necessitate developing adjoint solvers of the RANS model, which requires extra effort in code development and maintenance. Given this difficulty, we present an ensemble Kalman method with an adaptive step size to train a neural network-based turbulence model by using indirect observation data. To our knowledge, this is the first such attempt in turbulence modelling. The ensemble method is first verified on the flow in a square duct, where it correctly learns the underlying turbulence models from velocity data. Then, the generalizability of the learned model is evaluated on a family of separated flows over periodic hills. It is demonstrated that the turbulence model learned in one flow can predict flows in similar configurations with varying slopes.},
	urldate = {2024-05-17},
	author = {Zhang, Xin-Lei and Xiao, Heng and Luo, Xiaodong and He, Guowei},
	year = {2022},
	note = {Publisher: [object Object]
Version Number: 4},
	keywords = {type: Journal_Article, model: Turbulence_Modelling, domain: Physical_Hybrid_Models, evaluation:MSE,Fluid Dynamics (physics.flu-dyn), FOS: Physical sciences},
}
